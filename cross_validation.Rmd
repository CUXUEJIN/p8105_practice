---
title: "Untitled"
author: "xj2249"
date: "11/12/2019"
output: html_document
---

```{r setup, include=FALSE}
library(tidyverse)
library(modelr)
library(mgcv)
set.seed(1)
```

generate a dataset(non-linear)
```{r}
nonlin_df = 
  tibble(
    id = 1:100,
    x = runif(100, 0, 1),
    y = 1 - 10 * (x - .3) ^ 2 + rnorm(100, 0, .3)
  )

nonlin_df %>% 
  ggplot(aes(x = x, y = y)) + 
  geom_point() + geom_smooth(color = "yellow") + theme_bw()

```


training and test split
```{r}
train_df = sample_n(nonlin_df, 80)
# another way to do it 
train_df = sample_frac(nonlin_df, 0.8)

test_df = anti_join(nonlin_df, train_df, by = "id")


ggplot(train_df, aes(x = x, y = y)) + 
  geom_point() + 
  geom_point(data = test_df, color = "red")
```


fit three models of varying goodness
```{r}
linear_mod = lm(y ~ x, data = train_df)
smooth_mod = mgcv::gam(y ~ s(x), data = train_df)
wiggly_mod = mgcv::gam(y ~ s(x, k = 30), sp = 10e-6, data = train_df) #add k and sp to make it not work
```

```{r}
# totally missing
train_df %>% 
        add_predictions(linear_mod) %>% 
        ggplot(aes( x = x,  y = y)) +
        geom_point() + 
        geom_line(aes(y = pred),color = "red",size = 5) +
        geom_smooth(method = "lm",se = F,color = "blue",alpha = 0.5)

# they are the same!

# well done
train_df %>% 
        add_predictions(smooth_mod) %>% 
        ggplot(aes( x = x,  y = y)) +
        geom_point() + 
        geom_line(aes(y = pred),color = "red")   

# doing too much 
train_df %>% 
        add_predictions(wiggly_mod) %>% 
        ggplot(aes( x = x,  y = y)) +
        geom_point() + 
        geom_line(aes(y = pred),color = "red") 

# make three pic above into one! first try facet
train_df %>% 
  gather_predictions(linear_mod, smooth_mod, wiggly_mod) %>% 
  ggplot(aes(x = x, y = y)) +
  geom_point() +
  geom_line(aes(y = pred)) +
  facet_grid(.~model)

# try color
train_df %>% 
  gather_predictions(linear_mod, smooth_mod, wiggly_mod) %>% 
  ggplot(aes(x = x, y = y)) +
  geom_point() +
  geom_line(aes(y = pred, color = model))
# YES! this one is better than facet.

# feels like gather is better than add, it add a "model" variable
train_df %>% 
  gather_predictions(linear_mod) 
```

no statistical method to test which is better
therefore, in this context, we can use *cross validation* !!!
make prediction and compare RMSE
```{r}
rmse(linear_mod,test_df)
rmse(smooth_mod,test_df)
rmse(wiggly_mod,test_df)


# try to do it 
l = list(linear_mod,smooth_mod,wiggly_mod)
map(.x = l,~rmse(.x,test_df))
map(.x = l,~rmse(.x,train_df))

```

## pick the one that do better in the test dataset
because you already how good you model is for the training dataset.


not split by hand and try to do it more than once
so ...
# do this all using `modelr`
```{r}
cv_df <-
        crossv_mc(nonlin_df,100)

nonlin_df %>%  crossv_kfold(k = 20)
?crossv_kfold
str(cv_df %>% pull(train) %>% .[[1]])

cv_df %>% pull(train) %>% .[[1]] %>% as_tibble

a = crossv_mc(nonlin_df,100)
a %>% pull(train) %>% .[[1]] %>% as_tibble

# doesn't work this way(this seems to be a vecto??) list?
a[1,] %>%  pull(train) %>% as_tibble()


```


```{r}
cv_df =
  cv_df %>% 
  mutate(
    train = map(train, as_tibble),
    test = map(test, as_tibble))      
```

# try to fit linear model to every traing dataset
```{r}
# map allow you to have two argument
# more than two, ask youself do you really have to do it? or try pmap
cv_df = 
  cv_df %>% 
  mutate(linear_mod  = map(train, ~lm(y ~ x, data = .x)),
         rmse_lin = map2(.x = linear_mod, .y = test, ~rmse(.x,.y)),
         smooth_mod  = map(train, ~mgcv::gam(y ~ s(x), data = .x)),
         rmse_smooth = map2(.x = smooth_mod, .y = test, ~rmse(.x,.y)),
         wiggly_mod  = map(train, ~gam(y ~ s(x, k = 30), sp = 10e-6, data = .x)),
         rmse_wiggly = map2(wiggly_mod, test, ~rmse(model = .x, data = .y)))

cv_df = 
  cv_df %>% 
  mutate(linear_mod  = map(train, ~lm(y ~ x, data = .x)),
         smooth_mod  = map(train, ~mgcv::gam(y ~ s(x), data = .x)),
         wiggly_mod  = map(train, ~gam(y ~ s(x, k = 30), sp = 10e-6, data = .x))) %>% 
  mutate(rmse_linear = map2_dbl(linear_mod, test, ~rmse(model = .x, data = .y)),
         rmse_smooth = map2_dbl(smooth_mod, test, ~rmse(model = .x, data = .y)),
         rmse_wiggly = map2_dbl(wiggly_mod, test, ~rmse(model = .x, data = .y)))
```

```{r}
cv_df %>% 
  select(starts_with("rmse")) %>% 
  pivot_longer(
    everything(),
    names_to = "model", 
    values_to = "rmse",
    names_prefix = "rmse_") %>% 
  mutate(model = fct_inorder(model)) %>% 
  ggplot(aes(x = model, y = rmse)) + geom_violin()
```

therefore, basically , smooth model works better overall. and we prefer it rather than other two models.

have fun!
try a new dataset!
```{r}
child_growth <-
  read_csv("./data/nepalese_children.csv")
```

scatter plot
```{r}
child_growth %>% 
  ggplot(aes(x = weight , y = armc)) + 
  geom_point(alpha = 0.5)
```

piecewise
```{r}
child_growth =
  child_growth %>% 
  mutate(weight_cp = (weight > 7) * (weight - 7))
```

